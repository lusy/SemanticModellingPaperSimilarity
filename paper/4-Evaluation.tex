\section{Evaluation}

\subsection{\textbf{Komplexitätsanalyse des Algorithmus}}
Wie in Kapitel \ref{subsec:simMeasure} erläutert, kann der rekursive Ähnlichkeitsalgorithmus als ein iterativer, zum Fixpunkt konvergierender Algorithmus dargestellt werden.
Algorithmus \ref{alg:derAlg} veranschaulicht das iterative Verfahren für die Berechnung der semantischen Ähnlichkeit in einem Publikationsnetzwerk $G$.
Die Bestimmung der Parameter für den Algorithmus erfolgt in Kapitel \ref{subsec:params}.


\begin{program2}
%\begin{algorithmic}
\[
\begin{array}{ll}
\textbf{Input:} & \text{Publikationsnetzwerk \textit{G};}\\
& \lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5 \text{- Gewichtungen der Keywords-, Autoren-, Quellen-,}\\
& \text{Zitation- und Erscheinungsjahrrelationen eines Papers,} \sum_{i=1}^{5} \lambda_i = 1;\\
& \text{der Dämpfungsfaktor \textit{c};}\\
& \text{und die Anzahl der Iterationen \textit{k}}\\
\textbf{Output:} & \text{Der Ähnlichkeitswert } s(a,b), \forall a,b \in G \text{ und \textit{a}, \textit{b} Knoten der selben Art}
\end{array}
\]
\caption{Der iterative Algorithmus}
\label{alg:derAlg}
\end{program2}
\begin{Verbatim}[commandchars=\\\{\}, fontsize=\small, numbers=left, numbersep=3pt,codes={\catcode`$=3\catcode`_=8}]
\textbf{foreach} \begin{math}a{\in}G\end{math} \textbf{do} \hspace{5cm}\textbf{/* Initialization */}\\
    \textbf{foreach} \begin{math}b{\in}G\end{math} \textbf{do}\\
        \textbf{if} \begin{math}a==b\end{math} \textbf{then} \begin{math}R(a,b)=1\end{math}\\
        \textbf{else} \begin{math}R(a,b)=0\end{math}\\
\textbf{while} \begin{math}(k>0)\end{math} \textbf{do} \hspace{6.2cm}\textbf{/* Iteration */}\\
    \begin{math}k{\longleftarrow}k-1 \end{math}\\
    \textbf{foreach} \begin{math}a{\in}G\end{math} \textbf{do}\\
        \textbf{foreach} \begin{math}b{\in}G\end{math} \textbf{do}\\
            \textbf{if} \begin{math}a[Class]{==}b[Class]\end{math} \textbf{and} \begin{math}a[Class]{==}"Publication"\end{math} \textbf{then}\\
                \begin{math}key{\longleftarrow}0 \end{math}\\
                \textbf{foreach} \begin{math}k_a{\in}K(a)\end{math} \textbf{do}\\
                    \textbf{foreach} \begin{math}k_b{\in}K(b)\end{math} \textbf{do}\\
                        \begin{math}key{\longleftarrow}key{+}R(k_a,k_b)\end{math}\\
                \begin{math}R_{new}(a,b){\longleftarrow}\lambda_1*\cfrac{c*key}{|K(a)||K(b)|} \end{math}\\

                \begin{math}auth{\longleftarrow}0 \end{math}\\
                \textbf{foreach} \begin{math}a_a{\in}A(a)\end{math} \textbf{do}\\
                    \textbf{foreach} \begin{math}a_b{\in}A(b)\end{math} \textbf{do}\\
                        \begin{math}auth{\longleftarrow}auth{+}R(a_a,a_b)\end{math}\\
                \begin{math}R_{new}(a,b){+=}\lambda_2*\cfrac{c*auth}{|A(a)||A(b)|} \end{math}\\

                \begin{math}sour{\longleftarrow}0 \end{math}\\
                \textbf{foreach} \begin{math}s_a{\in}S(a)\end{math} \textbf{do}\\
                    \textbf{foreach} \begin{math}s_b{\in}S(b)\end{math} \textbf{do}\\
                        \begin{math}sour{\longleftarrow}sour{+}R(s_a,s_b)\end{math}\\
                \begin{math}R_{new}(a,b){+=}\lambda_3*\cfrac{c*sour}{|S(a)||S(b)|} \end{math}\\

                \begin{math}cit{\longleftarrow}0 \end{math}\\
                \textbf{foreach} \begin{math}c_a{\in}C(a)\end{math} \textbf{do}\\
                    \textbf{foreach} \begin{math}c_b{\in}C(b)\end{math} \textbf{do}\\
                        \begin{math}cit{\longleftarrow}cit{+}R(c_a,c_b)\end{math}\\
                \begin{math}R_{new}(a,b){+=}\lambda_4*\cfrac{c*cit}{|C(a)||C(b)|} \end{math}\\

                \begin{math}year{\longleftarrow}0 \end{math}\\
                \textbf{foreach} \begin{math}y_a{\in}Y(a)\end{math} \textbf{do}\\
                    \textbf{foreach} \begin{math}y_b{\in}Y(b)\end{math} \textbf{do}\\
                        \begin{math}year{\longleftarrow}year{+}R(y_a,y_b)\end{math}\\
                \begin{math}R_{new}(a,b){+=}\lambda_5*\cfrac{c*year}{|Y(a)||Y(b)|} \end{math}\\

            \textbf{else if} \begin{math}a[Class]{==}b[Class]\end{math} \textbf{and} \begin{math}a[Class]{!=}"Publication"\end{math} \textbf{then}\\
                \begin{math}link{\longleftarrow}0 \end{math}\\
                \textbf{foreach} \begin{math}l_a{\in}L(a)\end{math} \textbf{do}\\
                    \textbf{foreach} \begin{math}l_b{\in}L(b)\end{math} \textbf{do}\\
                        \begin{math}link{\longleftarrow}link{+}R(l_a,l_b)\end{math}\\
                \begin{math}R_{new}(a,b){+=}\cfrac{c*link}{|L(a)||L(b)|} \end{math}\\

            \textbf{else} pass\\
                /* a and b are from different classes */\\
    \textbf{foreach} \begin{math}a{\in}G\end{math} \textbf{do} \hspace{6.2cm}\textbf{/* Update */}\\
        \textbf{foreach} \begin{math}b{\in}G\end{math} \textbf{do}\\
            \begin{math}R(a,b){=}R_{new}(a,b)\end{math}\\
\textbf{return} \begin{math}R(*,*)\end{math}
\end{Verbatim}
%\caption{Der iterative Algorithmus}
%\end{program2}

%\begin{algorithm}
%\caption{your caption for this algorithm}
%\label{your label for references later in your document}
%\begin{algorithmic}
%\forall{$a \in G $}
%\end{algorithmic}
%\end{algorithm}

% Beschreiben bisschen
% Theoretische Grenzen: Speicher, Zeit
% wie siehts praktisch aus? symmetrische Sparsematrix (Python dictionary) mit nur den Nichtnull-Einträgen wird gespeichert
% Laufzeit: die Ähnlichkeit für verschiedenartige Knoten wird nicht weiter verfolgt, da wird auch was geprunnt.
%%----------- Eigentlich sind die Sachen unten für den Ausblick!
% wenn G zu groß, kann mans extern, außerhalb des Hauptspeichers auslagern (z.b. Graphdatenbank) (die Ähnlichkeitsmatrix, egal welche Struktur für ihre Repräsentation gewählt wird, wird erstmal einfachshalber im Arbeitsspeicher behalten
% das eigentliche (größere) Problem ist die Laufzeit -- es können Verfahren gesucht werden, um die zu optimieren / Teile des Algorithmus zu parallelisieren

Algorithmus \ref{alg:derAlg} wird entsprechend Definition \ref{def:semSim} initialisiert (Zeilen 1--4).
Für jedes Paar Knoten, die derselben Klasse angehören (jede zwei Publikationen, jede zwei Keywords, jede zwei Autoren, jede zwei Quellen und jede zwei Erscheinungsjahre),
wird dann in der (k+1)-te Iteration die Ähnlichkeit, $R_{new}(a,b)$, berechnet, indem die Ähnlichkeit für diese Knoten in der k-ten Iteration, $R(a,b)$, aktualisiert wird (Zeilen 7--45).
Bei der Berechnung der Ähnlichkeit zwischen zwei Publikationen fließen alle ihre Metadaten (Keywords, Autoren, Quelle, Zitationen und Erscheinungsjahr) ein (Zeilen 9--38).
Bei der Berechnung der Ähnlichkeit zwischen zwei Knoten jeder anderen Klasse fließen nur Publikationen ein, da Keywords, Autoren, Quellen und Erscheinungsjahre in dem modellierten Netzwerk nur Publikationen als Nachbarn haben (Zeilen 40--45).
Die Ähnlichkeit von zwei Knoten aus verschiedenen Klassen ist stets 0 und wird von dem Algorithmus nicht aktualisiert (Zeilen 47--48).
Für weitere Iterationen wird dann der Ähnlichkeitswert $R$ durch $R_{new}$ ersetzt (Zeilen 49--51).
\\
\\
Für ein Publikationsnetzwerk G mit n Knoten, ist die Speicherkomplexität des Algorithmus \ref{alg:derAlg} $\mathcal{O}(n^2)$.
Obwohl die dieser Arbeit zugrundeliegende Implementierung nicht die explizite $n{\times}n$ Matrix, sondern nur die Hälfte davon (die Matrix ist symmetrisch, $s(a,b){=}s(b,a)$) und davon nur die nichtnull-Einträge speichert, wird mit steigender Anzahl von Publikationen/Knoten insgesamt in G der Speicherbedarf quadratisch steigen.
\\
\\
Algorithmus \ref{alg:derAlg} hat im schlimmsten Fall eine Laufzeit von $\mathcal{O}(n^4)$, denn für die Ähnlichkeitsberechnung jeder zwei Knoten werden rekursiv auch alle Knotenpaare aus ihren Nachbarn betrachtet.
Selbst wenn ein Pruning basierend auf den Knotenarten durchgeführt wird (Ähnlichkeit für Knoten a und b, die nicht der selben Kategorie angehören, wird nicht berechnet), ändert sich die generelle Laufzeitkomplexität nicht.%oder??

\subsection{Bestimmung der Parameter}
\label{subsec:params}
Im Folgenden wird diskutiert, welche Auswirkung die Werte der Parameter $c$, $k$ und der Gewichtungen $\lambda_i$ haben und wie sie am geeignetsten gewählt werden können.
\\
\\
Der Dämpfungsfaktor $c$ bestimmt wie stark sich das globale Publikationsnetzwerk auf die Ähnlichkeit zwischen zwei konkreten Knoten $a$ und $b$ auswirkt.
Wenn $c$ relativ klein ist, tragen weiter entfernte von $a$ und $b$ Knoten viel weniger zu deren Ähnlichkeit bei.
Es wird nur die lokale Nachbarschaft von $a$ und $b$ in deren Ähnlichkeitsberechnung miteinbezogen.
Mit größerem $c$ fließt mehr von dem globalen Netzwerk bei der rekursiven Berechnung in den Ähnlichkeitswert zweier Knoten ein.
Demzufolge braucht der iterative Algorithmus bei größerem $c$ mehr Zeit zum Konvergieren.
\\
\\
Die Autoren von \cite{Lizorkin2010} beweisen für \textit{SimRank}, dass der Unterschied zwischen den theoretischen und iterativen Ähnlichkeitswerten für zwei Knoten $a$ und $b$ in der Anzahl der durchgeführten Iterationen exponentiell sinkt.
Sie stellen die folgende Ungleichung auf: $s(a,b)-R_k(a,b) \leq c^{k+1}$ .
Also bestimmt der Wert $c^{k+1}$ die Genauigkeit, die der iterative \textit{SimRank}-Algorithmus nach $k$ Iterationen erreicht.
Aufgrund dieser Schlussfolgerung wählt die vorliegende Arbeit die Werte $c=0.6$ und $k=7$ für die Berechnung der semantischen Ähnlichkeit.
Diese ergeben für den iterativen Algorithmus eine Genauigkeit von $0.6^{7+1}=0.0168$.
% Überzeugend erklären: Lässt sich auch hier anwenden, alternativ, Beweis im Appendix
\\
\\
Kapitel \ref{subsec:simMeasure} definiert bereits, dass die Summe der Gewichtungen aller verschiedenen Metadaten $1$ ergibt, $\sum_{i=1}^{5} \lambda_i = 1$.
Diese Normalisierung stellt sicher, dass kein Ähnlichkeitswert größer $1$ werden kann.
Diese Arbeit berechnet die semantsiche Ähnlichkeit mit drei verschiedenen Gewichtungen, die in Absprache mit Experten aus dem Konrad-Zuse-Zentrum zustande gekommen sind.
Im folgenden Kapitel wird ausgewertet, welche Gewichtung die besten Ergebnisse erzielt.
Die Experten argumentieren, dass die Bedeutung der einzelnen Metadaten für die Publikation davon abhänge, worauf Bibliotheken o.ä. Institutionen im Speziellen Wert legen und wie ihr eigener Use Case aussieht.
Für jede dieser Institutionen müssten die Gewichtungen für eine hinreichend große Datenmenge experimentell ermittelt werden.
Die Experten erwarten aber im durchschnittlichen Fall die besten Ergebnisse bei einer inhaltlichen Ausrichtung wie in Gewichtung 3.
Die $\lambda$-s sind in diesen Gewichtungen folgendermaßen verteilt:
\begin{table}[H]
\centering
\begin{tabular}{l c c c}
		%\hline
		\textbf{Parameter} &\textbf{Gewichtung 1} &\textbf{Gewichtung 2} &\textbf{Gewichtung 3} \\
		%\hline
		$\lambda_1$, Keywords & $0.5$ & $0.2$ & $0.4$\\
		$\lambda_2$, Autoren & $0.1$ & $0.2$ & $0.3$\\
		$\lambda_3$, Quellen & $0.1$ & $0.2$ & $0.1$\\
		$\lambda_4$, Zitationen & $0.2$ & $0.2$ & $0.1$\\
		$\lambda_5$, Erscheinungsjahre & $0.1$ & $0.2$ & $0.1$\\
	    %\hline
\end{tabular}
%\caption{Metadatengewichtungen}
\label{tab:parametrization}
\end{table}


\subsection{Testläufe}
Die strukturellen Ähnlichkeitsmaße \textit{SimRank}, \textit{P-Rank} und \textit{C-Rank} haben im schlimmsten Fall dieselbe Laufzeit- $\mathcal{O}(n^4)$ und Speicherkomplexität $\mathcal{O}(n^2)$ wie die hier vorgeschlagene semantische Ähnlichkeit \cite{simrank2002} \cite{ZhaoHS09} \cite{DBLP:journals/corr/abs-1109-1059}.
Sie wurden auf den folgenden Datensätzen angewandt.
Jeh und Widom werten \textit{SimRank} \cite{simrank2002} auf zwei Datensätzen mit jeweils $278,628$ und $1003$ Elementen aus.
Sie machen weder Angaben zu den Parametern des benutzten Systems noch zu der Zeit, die sie für die Durchführung der Berechnungen gebraucht haben.
Lizorkin et al., die diverse Ansätze für die Optimierung von \textit{SimRank} vorschlagen \cite{Lizorkin2010}, werten ebenfalls den nicht optimierten \textit{SimRank}-Algorithmus aus.
Auf ihrem Testsystem mit $1$Gb RAM und $2.1$GHz CPU braucht dieser bei $10,000$ Elementen 46 Stunden.
Die Autoren von \textit{P-Rank} wenden ihren Algorithmus auf drei Testdatensätze mit jeweils $218,930$, $21,740$ und $100,000$ Elementen an \cite{ZhaoHS09}.
Ihr Testsystem hatte $2$Gb RAM und $2.4$GHz CPU.
Man kann davon ausgehen, dass sie bei dieser Größe die Informationsnetzwerke extern halten mussten.
Sie machen ebenfalls keine Angaben zu der gebrauchten Zeit.
Yoon, Kim und Park werten \textit{C-Rank} \cite{DBLP:journals/corr/abs-1109-1059} auf einem Datensatz mit $23,795$ Publikationen aus.
Ihr System hatte $2.67$GHz CPU, Speicherangaben machen sie keine, auch keine Angaben zu der gebrauchten Zeit.

% Vergleich: Laufzeit und Speicherverbrauch der alten strukturellen Maße (SimRank, P-Rank, C-Rank)
% auf wie vielen Daten wurden diese ausgewertet und wie lange hats gedauert? was für parameter hatte das testsystem?
% -----------------
% Accuracy estimate of SimRank \cite{Lizorkin2010}: 10000 Nodes - 46 Stunden (1GB RAM, 2.1GHz CPU)

% P-Rank
%------
%Heterogenous IN: 218930 Nodes;
%Mit den beschriebenen Parametern ihres Systems (2.4 GHz CPU und 2 GB RAM) können sie auf keinen Fall alles im Memory laden...
%Bzw steht nirgendswo wie lange sie für gebraucht haben
%--------
%Homogenous IN: 21740 Nodes
% All our experiments are performed on an Intel PC with a 2.4GHz CPU, 2GB memory, running Redhat Fedora Core 4.
% One more experiment with syntetic data: 100 000 Nodes


%C-Rank: 23795 Publications
% All our experiments were performed on an Intel PC with Quad Core 2.67GHz CPU, running Windows 2008;
% RAM unknown
% time: unknown

Das System, auf dem der von dieser Arbeit vorgeschlagene Algorithmus für Berechnung der semantischen Ähnlichkeit zwischen Publikationen ausgeführt wird, verfügt über $2.93$GHz CPU und $14$Gb Arbeitsspeicher.
Die einzige Arbeit, die konkrete Hinweise für die Laufzeit der Algorithmen für strukturelle Ähnlichkeit in Informationsnetzwerken in der Praxis gibt, ist \cite{Lizorkin2010}.
Da diese Algorithmen die selbe Laufzeitkomplexität wie das hier vorgeschlagene Verfahren haben, kann man davon ausgehen, dass die Berechnung der Ähnlichkeiten für den gesamten \textit{zmath}-Datensatz mit $2,907,086$ Publikationen auf diesem Testsystem nicht praktikabel ist.
\\
\\
Aus diesem Grund wird der \textit{zmath}-Datensatz gekürzt.
Es werden nur die Publikationen betrachtet, für die die vollständigen Metadaten, die in die Berechnung der semantischen Ähnlichkeit einfließen, verfügbar sind.
Das ergibt $1,154,950$ Publikationen und insgesamt $5,518,500$ Knoten für das modellierte Publikationsnetzwerk.
Dieser Datensatz war für das gewählte Verfahren auf dem Testsystem erneut zu groß.
Um eine Auswertung der Ergebnisse des vorgeschlagenen Algorithmus durchführen zu können, werden nur noch die Publikationen bis einschließlich 1975 betrachtet.
Das ergibt ein Publikationsnetzwerk mit $3095$ Publikationen und $15,994$ Knoten insgesamt.
Für die Berechnung der Ähnlichkeiten wurden bei den oben beschriebenen Parametern ($k=7$ Iterationen und $c=0.6$ Dämpfungsfaktor) und Systemeigenschaften im Durchschnitt 4 Stunden gebraucht
(Die semantische Ähnlichkeit wurde für die drei verschiedenen Verteilungen der Gewichtungen $\lambda_i$, die in Kapitel \ref{subsec:params} beschrieben wurden, berechnet).

In dem verkürzten Datensatz gehen keine Zitationsrelationen zu Publikationen verloren, die außerhalb des Datensatzes liegen.
Die Publikationen, die bis zum Jahr 1975 veröffentlicht wurden, können keine Arbeiten zitieren, die nach diesem Datum erschienen.
Es muss allerdings beachtet werden, dass durch jede Verkürzung des ursprünglichen Datesatzes Relationen verloren gehen, die auch für die Ähnlichkeit der Publikationen, die im verkürzten Datensatz bleiben, von Bedeutung sind.
Das liegt an dem Wesen des vorgeschlagenen Algorithmus, der die Ähnlichkeit zwischen zwei Publikationen rekursiv aufgrund des gesamten Publikationsnetzwerks bestimmt.
Aus diesem Grund ist zu erwarten, dass die berechneten Ähnlichkeitswerte für die Publikationen bis zum Jahr 1975 nicht mit den Ähnlichkeitswerten eines kompletten Datensatzes von knapp 3 Mio Publikationen zusammenfallen.
% wie lange hat das gedauert??
% P1: 14911.1054752 Sec
% P2: 14732.8429248 Sec
% P3: 14984.688808 Sec
% Crank: 13935.943378 Sec

% So werden nicht Zitationsrelationen zu späteren Arbeiten verloren (eine frühere Arbeit kann eine spätere nicht zitieren)
% andererseit aber werden Relationen verloren, da der Algorithmus auf dem Gesamtpublikationsnetzwerk rechnet (also Werte, die über rausgeschnittenen Knoten propagiert wurden, gehen verloren)
% -- also eine Auswertung "wie gut ist das Ähnlichkeitsmaß" wird vermutlich nicht sehr sinnvolle Ergebnisse liefern


% Was genau habe ich ausgerechnet?

%Um das vorgeschlagene Ähnlichkeitsmaß für wissenschaftliche Publikationen auswerten zu können, wird der Testdatensatz vom Zentralblatt Mathematik auf das im Kapitel \ref{subsec:modell} vorgestellte Graphenschema abgebildet.
%Mit Hilfe von einem in \textit{Python} geschriebenen Parser werden die Daten vom \textit{zmath}-Datensatz in \textit{GraphML}-Format überführt.
%Die \textit{GraphML}-Repräsentation der vollständigen Daten ist ca. $4.8$ Gb groß.

% Parameters of the test system:
%Hardware Overview:
%    Model Name: Mac Pro
%    Model Identifier: MacPro4,1
%    Processor Name: Quad-Core Intel Xeon
%    Processor Speed: 2,93 GHz
%    Number Of Processors: 2
%    Total Number Of Cores: 8
%    L2 Cache (per core): 256 KB
%    L3 Cache (per processor): 8 MB
%    Memory: 14 GB
%    Processor Interconnect Speed: 6.4 GT/s

%System Software Overview:
%    System Version: Mac OS X 10.6.8 (10K549)
%    Kernel Version: Darwin 10.8.0
%    Boot Volume: Macintosh HD
%    Boot Mode: Normal

\subsection{Auswertung der Ergebnisse}
Es wurde die semantische Ähnlichkeit der \textit{zmath}-Publikationen bis 1975, für die alle Metadaten vorliegen, mit $c=0.6$, $k=7$ und den drei verschiedenen Gewichtungsverteilungen, wie in Kapitel \ref{subsec:params} beschrieben, berechnet.
Ferner wurden für denselben Datensatz die \textit{C-Rank}-Ähnlichkeitswerte berechnet.
Aufgrund der entstandenen Ähnlichkeitsmatrizen wurden die Ergebnisse aller vier Testdurchläufe nach dem Verfahren \textit{partitioning around medoids (pam)} \cite{pam2008}, eine Variante des \textit{k-Means} Clusterings, in 64 Cluster eingeclustert.
%vlt bisschen beschreiben was er macht.
Um die Qualität der Maße zu bestimmen wurden die bei jedem Maß/Parameterverteilung entstandenen Cluster mit den 64 Toplevel-MSC-Klassen\footnote{Alle MSC-Klassen wurden bis auf die ersten zwei Ziffern aggregiert. Z.B. werden Klassen 11A25, 11Axx und 11-XX als 11-XX betrachtet.},
die sich im Datensatz befanden, verglichen.
\\
\\
Für die Clusterings der drei Parametrisierungen des semantischen Ähnlichkeitsmaßes, sowie des \textit{C-Rank} wurden die \textit{Entropy}, \textit{Purity} und den \textit{Silhouette-Koeffizient} ermittelt.
Die \textit{Entropy} und \textit{Purity} sind Metriken der Supervised Cluster Validation \cite{dataMining2005}.
Wenn eine externe Klassifizierung der Objekte, auf die ein Cluteringverfahren angewandt werden sollte, vorliegt (in dem Fall die MSC-Klassifizierung), bestimmen die \textit{Entropy} und \textit{Purity} inwiefern die dabei entstandenen Cluster nur aus Elementen bestehen, die derselben externen Klasse angehören.
Der \textit{Purity}-Wert liegt im Intervall \begin{math}\left[0,1\right]\end{math}.
Je größer dieser für einen bestimmten Cluster ist, desto besser ist der Cluster.
Analog kann \textit{Purity} auch für das Gesamtclustering bestimmt werden.
% Gleichungen?
\\
\\
Die \textit{Entropy} ist ein Wert $\geq 0$, der den Grad der Unordnung eines Clusters beschreibt.
Je kleiner die \textit{Entropy} eines Clusters ist, desto besser ist der Cluster \cite{purityEntropy2011}.
\\
\\
Der \textit{Silhouette-Koeffizient} ist eine Metrik der Unsupervised Cluster Validation \cite{dataMining2005}.
Unsupervised Cluster Validation beurteilt inwiefern das Clustering an sich gut ist (d.h. ohne das Clustering mit einer externen Klassifizierung zu vergleichen), indem die Zusammengehörigkeit der Elemente innerhalb jedes Clusters und auch wie gut die einzelnen Cluster von einander abgegrenzt sind bemessen wird.
Der \textit{Silhouette-Koeffizient} wird für jedes Element des Datensatzes bestimmt, dann werden die Durchschnittswerte für jeden Cluster gebildet und zum Schluss wird er auch für das gesamte Clustering berechnet.
Der \textit{Silhouette-Koeffizient} $\in (-1,1)$ bestimmt inwiefern ein Element ähnlich allen anderen Elementen aus dem selben Cluster und gleichzeitig verschieden als alle Elemente aus anderen Clustern ist.
Ein negativer Wert dieses Koeffizienten bedeutet, dass das Element im Durchschnitt näher an Elementen ist, die außerhalb seinem Cluster sind, als an Elementen, die sich im selben Cluster befinden.
\\
\\
Diese Metriken wurden in Anlehnung an \cite{dataMining2005} berechnet und in Tabelle \ref{tab:results} für die drei Parametrisierungen des semantischen Ähnlichkeitsmaßes und für \textit{C-Rank} zusammengefasst.
Im Vergleich schneidet die semantische Ähnlichkeit bei allen Parametern besser als \textit{C-Rank} ab.
\textit{C-Rank} hat die größte \textit{Entropy} und die kleinsten \textit{Purity} und \textit{Silhouette}.
Ferner lässt sich feststellen, dass unter den drei verschiedenen Metadatengewichtungen der semantischen Ähnlichkeit die Zweite, bei der alle Metadaten gleich gewichtet waren, am schlechtesten ist.
Gewichtung 1 und Gewichtung 3 liefern ähnliche Resultate.
Gewichtung 3 hat bessere \textit{Entropy} und \textit{Purity}, also bildet das Clustering aufgrund der Distanzmatrix für Gewichtung 3 besser die MSC-Klassen ab.
Gewichtung 1 hat einen besseren \textit{Silhouette-Koeffizienten}, das heißt, die Cluster für Gewichtung 1 sind stärker zusammenhängend und deutlicher abgegrenzt.
Allerdings sind die Unterschiede nicht wesentlich. %oder??
\\
\\
\begin{table}[H]
\centering
\begin{tabular}{|l l l l l|}
		\hline
		 &\textbf{Gewichtung 1} &\textbf{Gewichtung 2} &\textbf{Gewichtung 3} & \textbf{C-Rank} \\
		\hline
		Entropy & $2.4863$ & $3.28742$ & $2.48117$ & $4.13098$\\
		Purity & $0.50953$ & $0.39774$ & $0.51599$ & $0.35767$\\
		Silhouette & $0.084$ & $0.056$ & $0.069$ & $0.0$ \\
	    \hline
\end{tabular}
\caption{Ergebnisse der Clustering Validation}
\label{tab:results}
\end{table}


Abbildung \ref{fig:entPur} stellt die Verteilungen der \textit{Entropy}- und \textit{Purity}-Werte für alle Metadatengewichtungen des semantischen Ähnlichkeitsmaßes und \textit{C-Rank} dar.
\\
\\
Auf dem ersten Blick scheinen die Werte für \textit{C-Rank} gut zu sein: mehrere Cluster haben eine hohe \textit{Purity} und niedrige \textit{Entropy}.
Allerdings werden die Werte für das Gesamtclustering nochmal gewichtet zusammengefasst: die Werte für größere Cluster fließen zu einem höheren Anteil in die \textit{Durchschnittsentropy} und -\textit{purity}.
Da der erste Cluster bei \textit{C-Rank} mit $Entropy{=}4.82283$ und $Purity{=}0.26131$ $2587$ der insgesamt $3095$ Publikationen beinhaltet, werden diese Werte ausschlaggebend für die \textit{Gesamtentropy} und \textit{-purity} des Clusterings.
Eine genauere Untersuchung der Distanzmatrix zeigt, dass dieses Phänomen auf die Tatsache zurückzuführen ist, dass \textit{C-Rank}, das nur auf Zitationen basiert, für die Mehrheit der Publikationen einen Ähnlichkeitswert von $0.0$ berechnet hat.
\\
\\
Bei den drei Metadatengewichtungen des semantischen Ähnlichkeitsmaßes sind die Clustergrößen viel homogener.
Deshalb würden im allgemeinen mehrere Cluster mit guten \textit{Entropy} und \textit{Purity} auch mehr zu den Gesamtwerten beitragen.

\smallskip

\begin{figure}[H]
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.3]{../evaluation/diss_true/crankPurityEntropy.png}
        \caption{C-Rank}
        \label{fig:crankEntPur}
    \end{subfigure}
    \qquad
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.3]{../evaluation/diss_true/p1PurityEntropy.png}
        \caption{Gewichtung 1}
        \label{fig:p1EntPur}
    \end{subfigure}
    \newline
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.3]{../evaluation/diss_true/p2PurityEntropy.png}
        \caption{Gewichtung 2}
        \label{fig:p2EntPur}
    \end{subfigure}
    \qquad
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.3]{../evaluation/diss_true/p3PurityEntropy.png}
        \caption{Gewichtung 3}
        \label{fig:p3EntPur}
    \end{subfigure}
    \caption{Verteilungen der Entropy und Purity für 64 Cluster aller Maße}
    \label{fig:entPur}
\end{figure}

%nochmal betonen, dass durch trimming informationen verloren gehen
%abschlussworte im sinne von blabla, iwas mit parameter ranking

% Macht das, was rauskommt, Sinn?
%\begin{figure}[hp]
%    \centering
%    \includegraphics[scale=0.6]{../evaluation/diss_true/p3_plot_pam_sil_diss.png}
%    \caption{lala}
%    \label{fig:silP3}
%\end{figure}

% Vergleich gegen die MSC-Klassen
% Vergleich mit einem rein bibliometrischen Verfahren (bibliographische Kopplung / SimRank/ ..)
% Entwickle 3 Varianten und vergleich sie: mit unterschiedlichen Gewichtung von den verschiedenen Relationen

% Clustering über die entstandene Ähnlichkeitsmatrix für alle Verfahren (C-Rank + alle 3 Parametrisierungen)
% Idee: Vergleich entstandene Cluster mit den ursprünglich vergebenen MSC-Klassen: wenn die MSC-Klassifizierung gut abgebildet, gutes Clustering
% MSC Klassen werden bis zur Top-Level Klassen aggregiert
% Wahl des Clusteringverfahrens

% Ergebnisse: Durchschnittswerte von Entropy, Purity, Silhouette-Koeffizient, Verteilung
%% Kurze Definition von Entropy, Purity und Silhouette
%% Beschreibung/Vergleich der Ergebnisse (für 3095 Publikationen und 64 Cluster)
% -- SemSim schneidet schon besser als C-Rank ab (C-Rank packt alles in den selben Cluster)
% -- Schlussfolgerungen: Entweder war das Clusteringverfahren doof oder aber ist das Trimming vom Datensatz dumm und es können keine adäquate Ergebnisse geliefert werden
