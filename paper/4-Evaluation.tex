\section{Evaluation}

\subsection{Komplexitätsanalyse des Algorithmus}
% Pseudo code
% speicher- und laufzeitkomplexität klären
% Laufzeit $\mathcal{O}(n^4)$ - 4 for-Schleifen
% Speicher $\mathcal{O}(n^2)$ - symmetrische Matrix, die die Ähnlichkeitswerte für jedes Paar (Publikationen/Knoten) speichert

\subsection{Testläufe}
% Vergleich: Laufzeit und Speicherverbrauch der alten strukturellen Maße (SimRank, P-Rank, C-Rank)
% auf wie vielen Daten wurden diese ausgewertet und wie lange hats gedauert? was für parameter hatte das testsystem?
% -----------------
% Accuracy estimate of SimRank \cite{Lizorkin2010}: 10000 Nodes - 46 Stunden (1GB RAM, 2.1GHz CPU)

% P-Rank
%------
%Heterogenous IN: 218930 Nodes;
%Mit den beschriebenen Parametern ihres Systems (2.4 GHz CPU und 2 GB RAM) können sie auf keinen Fall alles im Memory laden...
%Bzw steht nirgendswo wie lange sie für gebraucht haben
%--------
%Homogenous IN: 21740 Nodes
% All our experiments are performed on an Intel PC with a 2.4GHz CPU, 2GB memory, running Redhat Fedora Core 4.
% One more experiment with syntetic data: 100 000 Nodes


%C-Rank: 23795 Publications
% All our experiments were performed on an Intel PC with Quad Core 2.67GHz CPU, running Windows 2008;
% RAM unknown
% time: unknown



% Was genau habe ich ausgerechnet?

%Um das vorgeschlagene Ähnlichkeitsmaß für wissenschaftliche Publikationen auswerten zu können, wird der Testdatensatz vom Zentralblatt Mathematik auf das im Kapitel \ref{subsec:modell} vorgestellte Graphenschema abgebildet.
%Mit Hilfe von einem in \textit{Python} geschriebenen Parser werden die Daten vom \textit{zmath}-Datensatz in \textit{GraphML}-Format überführt.
%Die \textit{GraphML}-Repräsentation der vollständigen Daten ist ca. $4.8$ Gb groß.

% 1. Trimming des Datensatzes: auf $1$ Gb - nur die Publikationen mit vollständigen Metadaten, $1,154,950$ Publikationen
% -- nicht wirklich berechenbar in vernünftiger Zeit (siehe Komplexitätsanalyse)
% 2. Trimming des Datensatzes: 3095 Publikationen mit vollständigen Metadaten (alle Publikationen im Datensatz, die vor dem Jahr 1975 veröffentlicht wurden);
% wie lange hat das gedauert??
% P1: 14911.1054752 Sec
% P2: 14732.8429248 Sec
% P3: 14984.688808 Sec
% Crank: 13935.943378 Sec

% So werden nicht Zitationsrelationen zu späteren Arbeiten verloren (eine frühere Arbeit kann eine spätere nicht zitieren)
% andererseit aber werden Relationen verloren, da der Algorithmus auf dem Gesamtpublikationsnetzwerk rechnet (also Werte, die über rausgeschnittenen Knoten propagiert wurden, gehen verloren)
% -- also eine Auswertung "wie gut ist das Ähnlichkeitsmaß" wird vermutlich nicht sehr sinnvolle Ergebnisse liefern



% Parameters of the test system:
%Hardware Overview:
%    Model Name: Mac Pro
%    Model Identifier: MacPro4,1
%    Processor Name: Quad-Core Intel Xeon
%    Processor Speed: 2,93 GHz
%    Number Of Processors: 2
%    Total Number Of Cores: 8
%    L2 Cache (per core): 256 KB
%    L3 Cache (per processor): 8 MB
%    Memory: 14 GB
%    Processor Interconnect Speed: 6.4 GT/s

%System Software Overview:
%    System Version: Mac OS X 10.6.8 (10K549)
%    Kernel Version: Darwin 10.8.0
%    Boot Volume: Macintosh HD
%    Boot Mode: Normal

\subsection{Bestimmung der Parameter}
% wie werden die lambdas gewählt? warum?
% wie wird c gewählt? warum (simRank optimierungspaper)

% Notes on accuracy, decay factor and convergence:
% ------------------------------------------------

% It is obvious that the similarity score of C-Rank increases with the increase of C
% When C is low, the recursive power of C-Rank is weakened such that only the papers in local or near-local neighborhood are used in similarity computation.
% When C is high, more papers in a more global neighborhood can be used in computing the similarity recursively. When C is high, therefore, the convergence takes more time.
% \cite{Lizorkin2010} : folgende Gleichung gilt: s(a,b) - R_k(a,b) <= C^(k+1) gibt uns die Accuracy in Abhängigkeit von C und k
% wichtig wie das gewählt wird, Ähnlichkeitswerte sind zwischen 0 und 1, relativ hohe Genauigkeit ist wichtig!
% gewählte Parameter: c=0.6, k=7: mehr vom globalen Netz fließt in die Berechnung mit ein; erzielte Genauigkeit: 0.01679616
% (zum Vergleich SimRank nutzt originell c=0.8 und k= 5, was eine Genauigkeit von 0.26 ergibt, relativ ungenau!


\subsection{Auswertung der Ergebnisse}
% Macht das, was rauskommt, Sinn?

% Vergleich gegen die MSC-Klassen
% Vergleich mit einem rein bibliometrischen Verfahren (bibliographische Kopplung / SimRank/ ..)
% Entwickle 3 Varianten und vergleich sie: mit unterschiedlichen Gewichtung von den verschiedenen Relationen

% Clustering über die entstandene Ähnlichkeitsmatrix für alle Verfahren (C-Rank + alle 3 Parametrisierungen)
% Idee: Vergleich entstandene Cluster mit den ursprünglich vergebenen MSC-Klassen: wenn die MSC-Klassifizierung gut abgebildet, gutes Clustering
% MSC Klassen werden bis zur Top-Level Klassen aggregiert
% Wahl des Clusteringverfahrens

% Ergebnisse: Durchschnittswerte von Entropy, Purity, Silhouette-Koeffizient, Verteilung
%% Kurze Definition von Entropy, Purity und Silhouette
%% Beschreibung/Vergleich der Ergebnisse (für 3095 Publikationen und 64 Cluster)
% -- SemSim schneidet schon besser als C-Rank ab (C-Rank packt alles in den selben Cluster)
% -- Schlussfolgerungen: Entweder war das Clusteringverfahren doof oder aber ist das Trimming vom Datensatz dumm und es können keine adäquate Ergebnisse geliefert werden
