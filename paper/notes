Citation Statistics
-------------------

There is a belief that citation statistics are inherently 
more accurate because they substitute simple numbers for complex judgments, and hence overcome 
the possible subjectivity of peer review. But this belief is unfounded.

* Relying on statistics is not more accurate when the statistics are improperly used. Indeed, 
statistics can mislead when they are misapplied or misunderstood. Much of modern 
bibliometrics seems to rely on experience and intuition about the interpretation and validity of 
citation statistics.  
* While numbers appear to be "objective", their objectivity can be illusory. The meaning of a 
citation can be even more subjective than peer review. Because this subjectivity is less obvious 
for citations, those who use citation data are less likely to understand their limitations. 
• The sole reliance on citation data provides at best an incomplete and often shallow 
understanding of research—an understanding that is valid only when reinforced by other 
judgments. Numbers are not inherently superior to sound judgments

For papers, instead of relying on the actual count of citations to compare individual papers, 
people frequently substitute the impact factor of the journals in which the papers appear. They 
believe that higher impact factors must mean higher citation counts. But this is often not the 
case! This is a pervasive misuse of statistics that needs to be challenged whenever and wherever 
it occurs

Different citation behaviours in different fields!

// kurz erwähnen, ist eigentlich abseits vom hauptpunkt
For individual scientists, complete citation records can be difficult to compare. As a 
consequence, there have been attempts to find simple statistics that capture the full complexity 
of a scientist's citation record with a single number. The most notable of these is the h‐index, 
which seems to be gaining in popularity. But even a casual inspection of the h‐index and its 
variants shows that these are naïve attempts to understand complicated citation records. While 
they capture a small amount of information about the distribution of a scientist's citations, they 
lose crucial information that is essential for the assessment of research

We do not dismiss citation statistics as a tool for assessing the quality of research—citation data and 
statistics can provide some valuable information. We recognize that assessment must be practical, and 
for this reason easily‐derived citation statistics almost surely will be part of the process. But citation data 
provide only a limited and incomplete view of research quality, and the statistics derived from citation 
data are sometimes poorly understood and misused. Research is too important to measure its value 
with only a single coarse tool.  

The drive towards more transparency and 
accountability in the academic world has 
created a "culture of numbers" in which 
institutions and individuals believe that fair 
decisions can be reached by algorithmic 
evaluation of some statistical data; unable 
to measure quality (the ultimate goal), 
decision‐makers replace quality by numbers 
that they can measure. This trend calls for 
comment from those who professionally 
“deal with numbers”— mathematicians and 
statisticians

// Bestrebung ein objektives, messbares Maß zu finden

They believe that carefully chosen metrics are 
independent and free of bias. Most of all, they believe such metrics allow us to compare all parts of the 
research enterprise—journals, papers, people, programs, and even entire disciplines—simply and 
effectively, without the use of subjective peer review. 

Those who promote exclusive reliance on citation‐based metrics implicitly assume that each 
citation means the same thing about the cited research—its "impact". This is an assumption that 
is unproven and quite likely incorrect

Mathematicians know that there are many things, both real 
and abstract, that cannot be simply ordered, in the sense that each two can be compared. Comparison 
often requires a more complicated analysis, which sometimes leaves one undecided about which of two 
things is "better". The correct answer to "Which is better?" is sometimes: "It depends!" 

Citation‐based statistics can play a 
role in the assessment of research, provided they are used properly, interpreted with caution, and make 
up only part of the process

Journals that publish articles in languages other than English will likely receive fewer citations 
because a large portion of the scientific community cannot (or do not) read them. 

The only model derives from the impact factor itself—a larger impact factor means a better 
journal. ->ähmm noe
How does the impact factor measure quality? Is it the best statistic to measure quality? What precisely does it 
measure?
The impact factor cannot be used to compare journals across disciplines, for example, 
and one must look closely at the type of journals when using the impact factor to rank them.
One should 
also pay close attention to annual variations, especially for smaller journals,

 but there is 
a more fundamental and more insidious misuse: Using the impact factor to compare individual papers, 
people, programs, or even disciplines. 


impact factors used not only for retailing funds but also for deciding when/who is getting a promotion

Yet it is the interpretation of the statistics that leads to assessment, and the interpretation 
relies on the meaning of citations, which is quite subjective. 

"Underlying all these problems with the use of citations as a measure of quality is our ignorance 
of the reasons why authors cite particular pieces of work and not others. The problems 
described above ... Simple citation analysis presupposes a highly rational model of reference‐
giving, in which citations are held to reflect primarily scientific appreciation of previous work of 
high quality or importance, and potential citers all have the same chance to cite particular 
papers..." [Martin‐Irvine 1983, p. 69] 
Martin, Ben R., Irvine, John. 1983. Assessing basic research. Research Policy, Vol 12 (1983), pp. 61‐90. 
http://dx.doi.org/10.1016/0048‐7333(83)90005‐7  


some citations are "rhetorical": explains a result, probably not result of the current paper at all
or argues against it - no acknoledgement, whatsoever

 Because unlike "reward" citations, which tend to refer to seminal papers, the choice of 
which paper to cite rhetorically depends on many factors—the prestige of the cited author (the "halo" 
effect), the relationship of the citing and cited authors, the availability of the journal (Are open access 
journals more likely to be cited?), the convenience of referencing several results from a single paper, 
and so forth. Few of these factors are directly related to the "quality" of the cited paper. 

ariety of motives, including "currency, 
negative credit, operational Information, persuasiveness, positive credit, reader alert, and social 
consensus". [Brooks 1996]

"No amount of fancy statistical footwork will overcome basic inadequacies in either the 
appropriateness or the integrity of the data collected." [Goldstein‐Spiegelhalter 1996, p. 389] 

 A circular process seems to rank objects higher because they are ranked higher (in the 
database).







